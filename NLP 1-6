{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1tspwX-g9G_Ysp2hOAwYQ0kusy2FKeFU5","authorship_tag":"ABX9TyNXzaE+cdsYZ2vm4LfQX8ue"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import re\n","pattern1 = re.compile(r'abc')\n","result1 = pattern1.match('abcdef')\n","if result1:\n","    print(\"Match found:\", result1.group())\n","\n","pattern2 = re.compile(r'.')\n","result2 = pattern2.search('Hello')\n","if result2:\n","    print(\"Character found:\", result2.group())\n","pattern3 = re.compile(r'[aeiou]')\n","result3 = pattern3.search('Hello')\n","if result3:\n","    print(\"Vowel found:\", result3.group())\n","\n","pattern4 = re.compile(r'\\d{3}-\\d{2}-\\d{4}')\n","result4 = pattern4.match('123-45-6789')\n","if result4:\n","    print(\"Social Security Number:\", result4.group())\n","pattern5 = re.compile(r'\\.')\n","result5 = pattern5.search('www.example.com')\n","if result5:\n","    print(\"Dot found:\", result5.group())\n","\n","pattern6 = re.compile(r'(\\d{2})/(\\d{2})/(\\d{4})')\n","result6 = pattern6.match('01/09/2024')\n","if result6:\n","    print(\"Day:\", result6.group(1))\n","    print(\"Month:\", result6.group(2))\n","    print(\"Year:\", result6.group(3))\n","pattern7 = re.compile(r'cat|dog')\n","result7 = pattern7.search('I have a cat and a dog')\n","if result7:\n","    print(\"Animal found:\", result7.group())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3UeFmXpLHsPc","executionInfo":{"status":"ok","timestamp":1753944664334,"user_tz":-330,"elapsed":8,"user":{"displayName":"KURUPATI RANJITHA,AI & DATA SCIENCE(2022) Vel Tech, Chennai","userId":"05063237519581926606"}},"outputId":"848a18c6-5d78-4529-9926-40c4ba587a0a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Match found: abc\n","Character found: H\n","Vowel found: e\n","Social Security Number: 123-45-6789\n","Dot found: .\n","Day: 01\n","Month: 09\n","Year: 2024\n","Animal found: cat\n"]}]},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","text = \"Tokenization without transformers is straight forward with tools likeNLTK\"\n","tokens = word_tokenize(text)\n","print(\"Tokens:\", tokens)\n","from transformers import AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","text = \"Tokenization without transformers is straight forward with tools like NLTK\"\n","tokens_transformers = tokenizer(text, return_tensors = 'pt')\n","print(\"Transformers Tokens:\", tokens_transformers)\n","tokens_transformers_list = tokenizer.convert_ids_to_tokens(tokens_transformers['input_ids'][0].numpy().tolist())\n","print(\"Transformers Tokens(List):\", tokens_transformers_list)\n","decoded_text = tokenizer.decode(tokens_transformers['input_ids'][0], skip_special_tokens = True)\n","print(\"Decoded Text:\", decoded_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AsyGgwZ6-0Up","executionInfo":{"status":"ok","timestamp":1754544191188,"user_tz":-330,"elapsed":402,"user":{"displayName":"KURUPATI RANJITHA,AI & DATA SCIENCE(2022) Vel Tech, Chennai","userId":"05063237519581926606"}},"outputId":"3f224f16-53d9-42f8-c65d-d44f65f39572"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Tokens: ['Tokenization', 'without', 'transformers', 'is', 'straight', 'forward', 'with', 'tools', 'likeNLTK']\n","Transformers Tokens: {'input_ids': tensor([[  101, 19204,  3989,  2302, 19081,  2003,  3442,  2830,  2007,  5906,\n","          2066, 17953,  2102,  2243,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n","Transformers Tokens(List): ['[CLS]', 'token', '##ization', 'without', 'transformers', 'is', 'straight', 'forward', 'with', 'tools', 'like', 'nl', '##t', '##k', '[SEP]']\n","Decoded Text: tokenization without transformers is straight forward with tools like nltk\n"]}]},{"cell_type":"markdown","source":["task3"],"metadata":{"id":"kan_CR26FjLU"}},{"cell_type":"code","source":["import numpy as np\n","import nltk\n","from nltk.tokenize import word_tokenize\n","import spacy\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","spacy.cli.download(\"en_core_web_sm\")\n","nlp = spacy.load(\"en_core_web_sm\")\n","corpus = \"\"\"One disadvantage of using 'Best Of' samping is that it may lead to limited exploration of the model's\n","knowledge and creativity. By focusing on the most probable next words, the model might generate responses that are\n","safe and conventional, potentially missing out on more diverse and innovative outputs. The lack of exploration could\n","result in repetitive or less imaginative responses, especially in situations where novel and unconventional ideas are\n","desired.To address this limitation, other sampling strategies like temperature-based sampling or top-p (nucleus) sampling\n","can be employed to introduce more randomness and encourage the model to explore a broader range of possibilities.\n","However, it's essential to carefully balance exploration and exploitation based on the specific requirements of the task or\n","application.\"\"\"\n","tokens = word_tokenize(corpus)\n","lemmatized_tokens = [token.lemma_ for token in nlp(corpus)]\n","all_tokens = tokens + lemmatized_tokens\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(all_tokens)\n","total_words = len(tokenizer.word_index) + 1\n","input_sequences = []\n","for line in all_tokens:\n"," token_list = tokenizer.texts_to_sequences([line])[0]\n"," for i in range(1, len(token_list)):\n","  n_gram_sequence = token_list[:i+1]\n","  input_sequences.append(n_gram_sequence)\n","max_sequence_length = max(len(seq) for seq in input_sequences)\n","input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n","X, y = input_sequences[:, :-1], input_sequences[:, -1]\n","y = np.array(y)\n","model = Sequential()\n","model.add(Embedding(total_words, 100, input_length=max_sequence_length-1))\n","model.add(LSTM(100))\n","model.add(Dense(total_words, activation='softmax'))\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","model.fit(X, y, epochs=10, verbose=1)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3I3hINVTG7I4","executionInfo":{"status":"ok","timestamp":1754546331443,"user_tz":-330,"elapsed":13248,"user":{"displayName":"KURUPATI RANJITHA,AI & DATA SCIENCE(2022) Vel Tech, Chennai","userId":"05063237519581926606"}},"outputId":"01489a86-c9dd-4bed-adeb-229bbcb07b6e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.0000e+00 - loss: 4.6702\n","Epoch 2/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.3333 - loss: 4.6623\n","Epoch 3/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 1.0000 - loss: 4.6542\n","Epoch 4/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 1.0000 - loss: 4.6462\n","Epoch 5/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 1.0000 - loss: 4.6379\n","Epoch 6/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 1.0000 - loss: 4.6296\n","Epoch 7/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 1.0000 - loss: 4.6210\n","Epoch 8/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 1.0000 - loss: 4.6121\n","Epoch 9/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 1.0000 - loss: 4.6030\n","Epoch 10/10\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 1.0000 - loss: 4.5935\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x7c9bbf309ad0>"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["task4"],"metadata":{"id":"3FCtBqmMGHCT"}},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.probability import FreqDist\n","from docx import Document\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('punkt_tab')\n","\n","def load_document(file_path):\n","    doc = Document(file_path)\n","    full_text = []\n","    for para in doc.paragraphs:\n","        full_text.append(para.text)\n","    return '\\n'.join(full_text)\n","\n","def tokenize_document(document):\n","    tokens = word_tokenize(document)\n","    return [word.lower() for word in tokens if word.isalpha()]\n","\n","def remove_stopwords(tokens):\n","    stop_words = set(stopwords.words('english'))\n","    return [word for word in tokens if word not in stop_words]\n","\n","def find_morphology(tokens):\n","    fdist = FreqDist(tokens)\n","    return fdist.most_common(10)\n","\n","\n","document_path =  '/text-to-word.docx'\n","document = load_document(document_path)\n","\n","tokens = tokenize_document(document)\n","tokens_without_stopwords = remove_stopwords(tokens)\n","morphology = find_morphology(tokens_without_stopwords)\n","\n","\n","print(\"Top 10 frequent words in the document (after removing stopwords):\")\n","for word, frequency in morphology:\n","    print(f\"{word}: {frequency}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qyr9HIHTGD13","executionInfo":{"status":"ok","timestamp":1755754555441,"user_tz":-330,"elapsed":3645,"user":{"displayName":"KURUPATI RANJITHA,AI & DATA SCIENCE(2022) Vel Tech, Chennai","userId":"05063237519581926606"}},"outputId":"2f6ba990-20a0-4f1c-fa5d-029d227f9b1b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Top 10 frequent words in the document (after removing stopwords):\n","language: 3\n","nlp: 3\n","human: 2\n","machine: 2\n","learning: 2\n","natural: 1\n","processing: 1\n","field: 1\n","artificial: 1\n","intelligence: 1\n"]}]},{"cell_type":"markdown","source":["task5"],"metadata":{"id":"jFy_nqvEGOuL"}},{"cell_type":"code","source":["import string\n","import random\n","import nltk\n","from nltk.corpus import stopwords, reuters\n","from collections import Counter, defaultdict\n","from nltk import FreqDist, ngrams\n","\n","nltk.download('punkt_tab') # Download punkt_tab for sentence tokenization\n","nltk.download('stopwords') # Download stopwords\n","sents = reuters.sents()\n","stop_word = set(stopwords.words('english'))\n","string.punctuation = string.punctuation + ' \" ' + ' \" ' + ' - ' + ' _ '\n","removal_list = list(stop_word) + list(string.punctuation) + ['\\t', 'rt']\n","\n","unigram = []\n","bigram = []\n","trigram = []\n","tokenized_text = []\n","\n","for sentence in sents:\n","    # Convert words to lowercase\n","    sentence = [word.lower() for word in sentence]\n","    # Remove '.' from sentence\n","    sentence = [word for word in sentence if word != '.']\n","\n","    for word in sentence:\n","        unigram.append(word)\n","        tokenized_text.append(word)\n","\n","    # Generate bigrams and trigrams, padding with None\n","    bigram.extend(list(ngrams(sentence, 2, pad_left=True, pad_right=True)))\n","    trigram.extend(list(ngrams(sentence, 3, pad_left=True, pad_right=True)))\n","\n","def remove_stopwords(x):\n","    y = []\n","    for item in x:\n","        # Check if item is a tuple (for bigrams/trigrams) or a string (for unigrams)\n","        if isinstance(item, tuple):\n","            # If any word in the n-gram is not a stopword, keep the n-gram\n","            if any(word not in removal_list for word in item if word is not None):\n","                 y.append(item)\n","        elif isinstance(item, str):\n","             # If the unigram is not a stopword, keep it\n","            if item not in removal_list:\n","                y.append(item)\n","    return y\n","\n","unigram = remove_stopwords(unigram)\n","bigram = remove_stopwords(bigram)\n","trigram = remove_stopwords(trigram)\n","\n","\n","freq_uni = FreqDist(unigram)\n","freq_bi = FreqDist(bigram)\n","freq_tri = FreqDist(trigram)\n","\n","d = defaultdict(Counter)\n","for a, b, c in freq_tri:\n","    if (a is not None) and (b is not None) and (c is not None):\n","        d[(a, b)][c] += freq_tri[(a, b, c)]\n","\n","def pick_word(counter):\n","    \"choose a random element\"\n","    if counter:\n","        return random.choice(list(counter.elements()))\n","    else:\n","        return None # Return None if counter is empty\n","\n","prefix = \"he\", \"is\"\n","print(\" \".join(prefix))\n","s = \" \".join(prefix)\n","\n","for i in range(19):\n","    # Check if the prefix exists in the dictionary before trying to get a suffix\n","    if prefix in d:\n","        suffix = pick_word(d[prefix])\n","        if suffix is not None:\n","            s = s + ' ' + suffix\n","            print(s)\n","            prefix = prefix[1], suffix\n","        else:\n","            # Handle cases where no suffix is found for the prefix\n","            print(f\"No suffix found for prefix: {prefix}\")\n","            break # Stop generating if no suffix is found\n","    else:\n","        print(f\"Prefix not found in trigram data: {prefix}\")\n","        break # Stop generating if prefix is not in the dictionary"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WSHhyk8gD2QM","executionInfo":{"status":"ok","timestamp":1755753805009,"user_tz":-330,"elapsed":39454,"user":{"displayName":"KURUPATI RANJITHA,AI & DATA SCIENCE(2022) Vel Tech, Chennai","userId":"05063237519581926606"}},"outputId":"a11b6c90-e119-457b-bd43-943b8924acbc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"stream","name":"stdout","text":["he is\n","he is offering\n","he is offering 100\n","he is offering 100 pesos\n","he is offering 100 pesos --\n","he is offering 100 pesos -- the\n","he is offering 100 pesos -- the first\n","he is offering 100 pesos -- the first oil\n","he is offering 100 pesos -- the first oil crisis\n","he is offering 100 pesos -- the first oil crisis american\n","he is offering 100 pesos -- the first oil crisis american petroleum\n","he is offering 100 pesos -- the first oil crisis american petroleum institute\n","he is offering 100 pesos -- the first oil crisis american petroleum institute would\n","he is offering 100 pesos -- the first oil crisis american petroleum institute would report\n","he is offering 100 pesos -- the first oil crisis american petroleum institute would report for\n","he is offering 100 pesos -- the first oil crisis american petroleum institute would report for 1986\n","he is offering 100 pesos -- the first oil crisis american petroleum institute would report for 1986 fell\n","he is offering 100 pesos -- the first oil crisis american petroleum institute would report for 1986 fell to\n","he is offering 100 pesos -- the first oil crisis american petroleum institute would report for 1986 fell to 390\n","he is offering 100 pesos -- the first oil crisis american petroleum institute would report for 1986 fell to 390 ,\n"]}]},{"cell_type":"markdown","source":["task6"],"metadata":{"id":"N4u9rx2hFdKs"}},{"cell_type":"code","source":["import nltk\n","from nltk.util import ngrams\n","from nltk.lm import Laplace\n","from nltk.tokenize import word_tokenize\n","from nltk.lm.preprocessing import padded_everygram_pipeline\n","\n","def ngram_smoothing(sentence, n):\n","    tokens = word_tokenize(sentence.lower())\n","    # We wrap tokens in a list to indicate a single sentence\n","    train_data, padded_sents = padded_everygram_pipeline(n, [tokens])\n","\n","    model = Laplace(n)\n","    model.fit(train_data, padded_sents)\n","    return model\n","\n","\n","sentence = input(\"Enter a sentence: \")\n","n = int(input(\"Enter the value of N for N-grams: \"))\n","\n","model = ngram_smoothing(sentence, n)\n","\n","\n","if n > 1:\n","    context = tuple(sentence.lower().split()[-(n-1):])\n","else:\n","    context = ()\n","\n","next_words = model.generate(3, text_seed=context)\n","print(\"Next words:\", ' '.join(next_words))"],"metadata":{"id":"kXZ4QncbFWOK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755757951135,"user_tz":-330,"elapsed":57038,"user":{"displayName":"KURUPATI RANJITHA,AI & DATA SCIENCE(2022) Vel Tech, Chennai","userId":"05063237519581926606"}},"outputId":"a8e7e8af-58c2-47d2-a072-a58e1f207f8e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter a sentence: natural language processing\n","Enter the value of N for N-grams: 1\n","Next words: natural natural natural\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"_Sp19RTWHkhB"},"execution_count":null,"outputs":[]}]}